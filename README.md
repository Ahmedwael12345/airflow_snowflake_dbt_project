# ğŸš€ End-to-End Data Engineering Project with dbt, Snowflake, and Apache Airflow

This project demonstrates a complete data engineering pipeline using **dbt Core** for SQL-based data transformation, **Snowflake** as a cloud data warehouse, and **Apache Airflow** for orchestration â€” all running inside Docker containers.

---

## ğŸ“¦ Tech Stack

- **Snowflake** â€“ Cloud-based data warehouse  
- **dbt Core** â€“ Data transformation tool using SQL & Jinja  
- **Apache Airflow** â€“ Workflow orchestrator  
- **PostgreSQL** â€“ Metadata DB for Airflow  
- **Docker & Docker Compose** â€“ Containerized development

---

## ğŸ“ Project Structure

end_to_end_project/
â”œâ”€â”€ dags/ # Airflow DAGs
â”œâ”€â”€ dbt/ # dbt project (models, configs)
â”œâ”€â”€ Dockerfile # Custom Airflow image
â”œâ”€â”€ docker-compose.yaml # Service definitions
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .env # Snowflake credentials
â””â”€â”€ README.md # You're here!




## âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
https://github.com/Ahmedwael12345/airflow_snowflake_dbt_project.git
cd end_to_end_project



2. Configure .env File

Create a .env file in the project root with:

SNOWFLAKE_USER=dbt_user
SNOWFLAKE_PASSWORD=dbt_password
SNOWFLAKE_ACCOUNT=fx56113.eu-central-2.aws
SNOWFLAKE_DATABASE=finance_db
SNOWFLAKE_SCHEMA=raw
SNOWFLAKE_ROLE=ACCOUNTADMIN
SNOWFLAKE_WAREHOUSE=finance_wh



Start the Project

docker-compose up --build

    Airflow UI: http://localhost:8088

    DAG ID: dbt_snowflake_workflow



ğŸ‘¨â€ğŸ’» Author

Ahmed Wael Galal
Email: ahmedwaelgalal2@gmail.com





